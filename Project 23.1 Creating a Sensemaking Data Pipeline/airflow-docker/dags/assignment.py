from airflow import DAG
from datetime import timedelta
from airflow.operators.bash import BashOperator
from airflow.utils.dates import days_ago
from airflow.operators.python import PythonOperator
import time
import glob
import os
import json
import urllib.request


# task 1 pull course catalog pages
def catalog():
    # pull course catalog pages
    def pull(url):
        try:
            response = urllib.request.urlopen(url).read()
            data = response.decode("utf-8")
            return data
        except:
            print("failed to pull: " + url)
            return None

    # store data from url into file
    def store(data, file):
        with open(file, "w") as f:
            f.write(data)
        print("stored file: " + file)

    # Read URLs from 00_urls.txt. Have to put the file in the same directory as the script. This is a shared volume, the container shares it with the host. You can see this by going in docker desktop, opening the airflow-worker-1 container terminal, opt/airflow
    file_path = os.path.join(os.path.dirname(__file__), "00_urls.txt")
    with open(file_path, "r") as f:
        urls = [line.strip() for line in f.readlines()]

    # Iterate through the URLs.
    # For some reason m10b.html failed, maybe MIT detected DDOS? When I manually go to the site, it works. I'll try to skip it.
    for url in urls:
        index = url.rfind("/") + 1
        data = pull(url)  # Call pull function
        file = url[index:]
        print("pulled: " + file)
        if data is None:
            continue
        store(data, file)  # Call store function
        print("--- waiting ---")
        # time.sleep(.1) # should sleep less than 15 seconds so it goes faster


# Task 2 Combine all unstructured data files into one large file
def combine():
    with open("combo.txt", "w") as outfile:
        for file in glob.glob("*.html"):
            with open(file, "r") as infile:
                outfile.write(infile.read())
                outfile.write(
                    "\n"
                )  # Add a newline to separate contents of different files


# Task 3 Extract titles from the large file
def titles():
    from bs4 import BeautifulSoup

    def store_json(data, file):
        with open(file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            print("wrote file: " + file)

    # Open and read the large html file generated by combine()
    with open("combo.txt", "r", encoding="utf-8") as f:
        html = f.read()

    # the following replaces new line and carriage return char
    html = html.replace("\n", " ").replace("\r", "")
    # the following create an html parser
    soup = BeautifulSoup(html, "html.parser")
    results = soup.find_all("h3")
    titles = []
    # tag inner text
    for item in results:
        titles.append(item.text)
    store_json(titles, "titles.json")


# Task 4 Clean the titles
def clean():
    # complete helper function definition below
    def store_json(data, file):
        with open(file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            print("wrote file: " + file)

    with open("titles.json") as file:
        titles = json.load(file)
        # remove punctuation/numbers
        for index, title in enumerate(titles):
            punctuation = """!()-[]{};:'"\,<>./?@#$%^&*_~1234567890"""
            translationTable = str.maketrans("", "", punctuation)
            clean = title.translate(translationTable)
            titles[index] = clean

        # remove one character words
        for index, title in enumerate(titles):
            clean = " ".join([word for word in title.split() if len(word) > 1])
            titles[index] = clean

        store_json(titles, "titles_clean.json")


# Task 5 Count the words
def count_words():
    from collections import Counter

    def store_json(data, file):
        with open(file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
            print("wrote file: " + file)

    with open("titles_clean.json") as file:
        titles = json.load(file)
        words = []

        # extract words and flatten
        for title in titles:
            words.extend(title.split())

        # count word frequency
        counts = Counter(words)
        store_json(counts, "words.json")


# define the DAG
with DAG(
    "assignment",
    start_date=days_ago(1),
    schedule_interval="@daily",
    catchup=False,
) as dag:

    # INSTALL BS4 BY HAND THEN CALL FUNCTION
    # ts are tasks
    t0 = BashOperator(
        task_id="install_task", bash_command="pip install beautifulsoup4", retries=2
    )
    t1 = PythonOperator(
        task_id="catalog", depends_on_past=False, python_callable=catalog
    )
    # define tasks from t2 to t5 below
    t2 = PythonOperator(
        task_id="combine_task", depends_on_past=False, python_callable=combine
    )
    t3 = PythonOperator(
        task_id="titles_task", depends_on_past=False, python_callable=titles
    )
    t4 = PythonOperator(
        task_id="clean_task", depends_on_past=False, python_callable=clean
    )
    t5 = PythonOperator(
        task_id="count_task", depends_on_past=False, python_callable=count_words
    )

    t0 >> t1 >> t2 >> t3 >> t4 >> t5
